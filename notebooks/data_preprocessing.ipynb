{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data Preprocessing for Student Performance, Breast Cancer, and Penguins Datasets\n",
    "\n",
    "\n",
    "\n",
    " This notebook preprocesses three datasets:\n",
    "\n",
    "\n",
    "\n",
    " - **Student_Performance.csv**: A regression dataset predicting `Performance Index`.\n",
    "\n",
    " - **breast-cancer.csv**: A classification dataset predicting `diagnosis`. Two target versions are produced:\n",
    "\n",
    "    - For logistic regression: target labels {0, 1}.\n",
    "\n",
    "    - For SVM: target labels {-1, 1}.\n",
    "\n",
    " - **penguins.csv**: A clustering dataset for a K-Means task. In this section, best practices are followed by:\n",
    "\n",
    "    - Handling missing values using the \"differentiated\" strategy (numeric columns are imputed with the median, categorical with the mode).\n",
    "\n",
    "    - Separating the ground‐truth label (`sex`) from the clustering features.\n",
    "\n",
    "    - Normalizing the numeric features (in the range [0,1])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Setup\n",
    "\n",
    " Import necessary libraries and set up the project root for file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Set project root directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Import custom utility functions from data_utils.py\n",
    "from src.scratch.utils.data_utils import (\n",
    "    load_data, \n",
    "    shuffle_data_pandas, \n",
    "    encode_categorical, \n",
    "    handle_missing_values, \n",
    "    feature_target_split, \n",
    "    normalize, \n",
    "    split_data,\n",
    "    drop_columns\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Student Performance Preprocessing (Regression)\n",
    "\n",
    "\n",
    "\n",
    " This section prepares the `Student_Performance.csv` dataset for predicting the `Performance Index`.\n",
    "\n",
    " Steps include shuffling, encoding categorical columns, handling missing values, feature‐target splitting, normalization, and splitting into training/test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Load and Inspect Data\n",
    "\n",
    " Load the dataset and display basic information to confirm structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Student Performance dataset and shuffle it\n",
    "data_path = Path(\"../data/raw/Regression_Dataset/Student_Performance.csv\")\n",
    "df_student = load_data(data_path)\n",
    "df_student = shuffle_data_pandas(df_student)\n",
    "\n",
    "# Display basic information and the first few rows\n",
    "print(\"Student Performance Dataset Info:\")\n",
    "print(df_student.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_student.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Encode Categorical Columns\n",
    "\n",
    "\n",
    "\n",
    " Here the `Extracurricular Activities` column (with values 'Yes'/'No') is encoded to numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_student = encode_categorical(df_student)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Handle Missing Values\n",
    "\n",
    " - From `df.info()`, there are no missing values (10,000 non-null entries per column).\n",
    "\n",
    " - Apply the function for completeness and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_student = handle_missing_values(df_student, strategy=\"mean\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Split Features and Target\n",
    "\n",
    " - **Target**: `Performance Index` (float64, continuous for regression).\n",
    "\n",
    " - **Features**: All other columns (`Hours Studied`, `Previous Scores`, `Extracurricular Activities`, `Sleep Hours`, `Sample Question Papers Practiced`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = \"Performance Index\"\n",
    "X_student, y_student = feature_target_split(df_student, target_column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Features and Convert to NumPy Arrays\n",
    "\n",
    "- Normalize only the feature columns (X) to ensure consistent scale.\n",
    "\n",
    "- Do not normalize the target (`Performance Index`) as it’s a regression output.\n",
    "\n",
    "- Convert features and target to NumPy arrays for compatibility with machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_student = normalize(X_student)\n",
    "X_student = X_student.to_numpy()\n",
    "y_student = y_student.to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Split into Training and Test Sets and Save\n",
    "\n",
    "\n",
    "\n",
    " The data is split 80/20 for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_student, X_test_student, y_train_student, y_test_student = split_data(X_student, y_student, test_size=0.2)\n",
    "\n",
    "np.save(\"../data/processed/student_X_train.npy\", X_train_student)\n",
    "np.save(\"../data/processed/student_X_test.npy\", X_test_student)\n",
    "np.save(\"../data/processed/student_y_train.npy\", y_train_student)\n",
    "np.save(\"../data/processed/student_y_test.npy\", y_test_student)\n",
    "\n",
    "print(\"\\nStudent Performance data processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Breast Cancer Preprocessing (Classification)\n",
    "\n",
    "\n",
    "\n",
    " This section prepares the `breast-cancer.csv` dataset for predicting the `diagnosis` label.\n",
    "\n",
    " Two target versions are produced:\n",
    "\n",
    "\n",
    "\n",
    " - **Logistic Regression Version**: Targets remain {0,1} (‘M’ is mapped to 1 and ‘B’ to 0).\n",
    "\n",
    " - **SVM Version**: Targets are transformed to {-1,1} (with 0 converted to -1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Load and Inspect Data\n",
    "\n",
    " Load the dataset and display basic information to confirm structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Breast Cancer dataset and shuffle it\n",
    "data_path = Path(\"../data/raw/Classification_Dataset/breast-cancer.csv\")\n",
    "df_bc = load_data(data_path)\n",
    "df_bc = shuffle_data_pandas(df_bc)\n",
    "\n",
    "# Display basic info and first few rows\n",
    "print(\"Breast Cancer Dataset Info:\")\n",
    "print(df_bc.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_bc.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Irrelevant Columns and Encode Target\n",
    "\n",
    "- `id` column is irrelevant for modeling and should be removed.\n",
    "- `diagnosis` is the target column (object type, 'M' for malignant, 'B' for benign).\n",
    "\n",
    "- Map 'M' to 1 and 'B' to 0 for binary classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bc = drop_columns(df_bc, [\"id\"])\n",
    "df_bc[\"diagnosis\"] = df_bc[\"diagnosis\"].map({\"M\": 1, \"B\": 0}).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Handle Missing Values\n",
    "\n",
    " - From `df.info()`, there are no missing values (569 non-null entries per column).\n",
    "\n",
    " - Apply the function for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bc = handle_missing_values(df_bc, strategy=\"mean\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Split Features and Target\n",
    "\n",
    " - **Target**: `diagnosis` (now int, binary for classification).\n",
    "\n",
    " - **Features**: All other columns (30 numerical features like `radius_mean`, `texture_mean`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = \"diagnosis\"\n",
    "X_bc, y_bc = feature_target_split(df_bc, target_column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Categorical Columns in Features (Safeguard) and Normalize\n",
    "\n",
    "- No categorical columns in features (all are float64 after dropping `id` and encoding `diagnosis`).\n",
    "\n",
    "- Apply the function as a safeguard for future datasets.\n",
    "\n",
    "- Normalize only the feature columns (X) to ensure consistent scale.\n",
    "\n",
    "- Do not normalize the target (`diagnosis`) as it’s a binary label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bc = encode_categorical(X_bc)\n",
    "X_bc = normalize(X_bc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Convert to NumPy Arrays\n",
    "\n",
    " - Convert features and target to NumPy arrays for model compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bc = X_bc.to_numpy()\n",
    "y_bc = y_bc.to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Split into Training and Test Sets and Create Two Versions of the Target\n",
    "\n",
    "\n",
    "\n",
    " The data is split 80/20. Additionally, an SVM version of the target is created by converting 0 to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bc, X_test_bc, y_train_bc, y_test_bc = split_data(X_bc, y_bc, test_size=0.2)\n",
    "\n",
    "# Create SVM targets by replacing 0 with -1\n",
    "y_train_bc_svm = np.where(y_train_bc == 0, -1, 1)\n",
    "y_test_bc_svm = np.where(y_test_bc == 0, -1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Save Processed Data for Both Logistic Regression and SVM\n",
    "\n",
    "\n",
    "\n",
    " The logistic regression version uses {0,1} targets. The SVM version uses {-1,1}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save logistic regression data\n",
    "np.save(\"../data/processed/breast_cancer_X_train.npy\", X_train_bc)\n",
    "np.save(\"../data/processed/breast_cancer_X_test.npy\", X_test_bc)\n",
    "np.save(\"../data/processed/breast_cancer_y_train.npy\", y_train_bc)\n",
    "np.save(\"../data/processed/breast_cancer_y_test.npy\", y_test_bc)\n",
    "\n",
    "# Save SVM-specific targets\n",
    "np.save(\"../data/processed/breast_cancer_y_train_svm.npy\", y_train_bc_svm)\n",
    "np.save(\"../data/processed/breast_cancer_y_test_svm.npy\", y_test_bc_svm)\n",
    "\n",
    "print(\"\\nBreast Cancer data (both logistic regression and SVM versions) processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Penguins Preprocessing (K-Means Clustering)\n",
    "\n",
    "\n",
    "\n",
    " The Penguins dataset is preprocessed for unsupervised clustering using K-Means.\n",
    "\n",
    " For best practice:\n",
    "\n",
    " - Missing values are handled using the \"differentiated\" strategy.\n",
    "\n",
    " - The ground-truth label (`sex`) is separated (to be used for later evaluation) and dropped from the clustering features.\n",
    "\n",
    " - The remaining numeric features are normalized to the [0, 1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Penguins dataset\n",
    "file_path = Path(\"../data/raw/K_Means_Dataset/penguins.csv\")\n",
    "df_penguins = load_data(file_path)\n",
    "\n",
    "# Optionally, shuffle the dataset\n",
    "df_penguins = shuffle_data_pandas(df_penguins)\n",
    "\n",
    "# Display basic info and the first few rows\n",
    "print(\"Penguins Dataset Info:\")\n",
    "print(df_penguins.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_penguins.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Handle Missing Values\n",
    "\n",
    "\n",
    "\n",
    " Use the \"differentiated\" strategy to:\n",
    "\n",
    " - Impute missing numeric values with the median.\n",
    "\n",
    " - Impute missing categorical values with the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penguins = handle_missing_values(df_penguins, strategy=\"differentiated\")\n",
    "\n",
    "# Verify that there are no missing values left\n",
    "print(\"Missing values after handling:\")\n",
    "print(df_penguins.isnull().sum())\n",
    "print(df_penguins.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Separate the Ground-Truth Label and Drop from Features\n",
    "\n",
    "\n",
    "\n",
    " Although K-Means is unsupervised, the `sex` column is available as a ground-truth label for later evaluation.\n",
    "\n",
    " Therefore, we:\n",
    "\n",
    " 1. Save the `sex` column separately.\n",
    "\n",
    " 2. Remove the `sex` column from the features used for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the ground-truth labels (sex)\n",
    "y_penguins = df_penguins[\"sex\"]\n",
    "\n",
    "# Drop the 'sex' column from the clustering features\n",
    "X_penguins = drop_columns(df_penguins, [\"sex\"])\n",
    "\n",
    "print(\"\\nFeatures used for clustering:\")\n",
    "print(X_penguins.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Ground-Truth Labels for Evaluation\n",
    "\n",
    "For consistent evaluation (e.g., when using evaluation metrics that require numeric labels),\n",
    "we encode the `sex` column as follows:\n",
    "\n",
    "- \"MALE\" is mapped to 0.\n",
    "- \"FEMALE\" is mapped to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_mapping = {\"MALE\": 0, \"FEMALE\": 1}\n",
    "y_penguins_encoded = y_penguins.map(sex_mapping)\n",
    "print(\"\\nEncoded ground-truth labels sample:\")\n",
    "print(y_penguins_encoded.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Normalize Features\n",
    "\n",
    "\n",
    "\n",
    " K-Means is sensitive to the scale of features. We normalize the numeric features to the [0, 1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_penguins_norm = normalize(X_penguins)\n",
    "print(\"\\nNormalized features sample:\")\n",
    "print(X_penguins_norm.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### (Optional) Split into Train and Test Sets\n",
    "\n",
    "\n",
    "\n",
    " Although clustering typically uses the full dataset, you may want to split the data\n",
    "\n",
    " to assess the stability of clusters. Here we perform an optional 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_penguins, X_test_penguins, y_train_penguins, y_test_penguins = split_data(X_penguins_norm, y_penguins, test_size=0.2)\n",
    "\n",
    "print(\"Shapes of splitted data:\")\n",
    "print(\"X_train:\", X_train_penguins.shape, \"X_test:\", X_test_penguins.shape)\n",
    "print(\"y_train:\", y_train_penguins.shape, \"y_test:\", y_test_penguins.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Save Processed Data\n",
    "\n",
    "\n",
    "\n",
    " We save:\n",
    "\n",
    " - The full normalized features (for clustering).\n",
    "\n",
    " - The encoded ground-truth labels (for later evaluation).\n",
    "\n",
    " - Optionally, the train/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the full normalized features and ground-truth labels\n",
    "np.save(\"../data/processed/processed_penguins_features.npy\", X_penguins_norm.to_numpy())\n",
    "np.save(\"../data/processed/processed_penguins_labels.npy\", y_penguins.to_numpy())\n",
    "\n",
    "# Save the optional train/test splits as well\n",
    "np.save(\"../data/processed/penguins_X_train.npy\", X_train_penguins.to_numpy())\n",
    "np.save(\"../data/processed/penguins_X_test.npy\", X_test_penguins.to_numpy())\n",
    "np.save(\"../data/processed/penguins_y_train.npy\", y_train_penguins.to_numpy())\n",
    "np.save(\"../data/processed/penguins_y_test.npy\", y_test_penguins.to_numpy())\n",
    "\n",
    "print(\"\\nPenguins dataset processed and saved for K-Means clustering.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
